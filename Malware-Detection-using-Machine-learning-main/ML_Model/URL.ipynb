{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78948ef8-821e-44bb-9e20-13c7a6033f19",
   "metadata": {},
   "source": [
    "#### Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d609ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      url label\n",
      "0  diaryofagameaddict.com   bad\n",
      "1        espdesign.com.au   bad\n",
      "2      iamagameaddict.com   bad\n",
      "3           kalantzis.net   bad\n",
      "4   slightlyoffcenter.net   bad\n",
      "Unique labels: {'good', 'bad'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai keerthan\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 96.20 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the path to the file\n",
    "url = r'C:\\Users\\Sai keerthan\\Downloads\\malware_detection_url\\Malware-Detection-using-Machine-learning-main\\Dataset\\data_url.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(url):\n",
    "    print(f\"Error: The file was not found at {url}\")\n",
    "else:\n",
    "    # Load the CSV file\n",
    "    url_df = pd.read_csv(url, delimiter=',', on_bad_lines='skip')\n",
    "\n",
    "    # Confirm the DataFrame structure\n",
    "    print(url_df.head())\n",
    "\n",
    "    # Extract URLs and labels explicitly by column name\n",
    "    urls = url_df['url'].tolist()\n",
    "    y = url_df['label'].tolist()\n",
    "\n",
    "    # Verify unique labels\n",
    "    print(\"Unique labels:\", set(y))\n",
    "\n",
    "    def sanitization(web):\n",
    "        web = web.lower()\n",
    "        token = []\n",
    "        dot_token_slash = []\n",
    "        raw_slash = str(web).split('/')\n",
    "        for i in raw_slash:\n",
    "            # removing slash to get token\n",
    "            raw1 = str(i).split('-')\n",
    "            slash_token = []\n",
    "            for j in range(0, len(raw1)):\n",
    "                # removing dot to get the tokens\n",
    "                raw2 = str(raw1[j]).split('.')\n",
    "                slash_token = slash_token + raw2\n",
    "            dot_token_slash = dot_token_slash + raw1 + slash_token\n",
    "        # to remove same words\n",
    "        token = list(set(dot_token_slash))  \n",
    "        if 'com' in token:\n",
    "            #remove com\n",
    "            token.remove('com')\n",
    "        return token\n",
    "\n",
    "    # term-frequency and inverse-document-frequency\n",
    "    vectorizer = TfidfVectorizer(tokenizer=sanitization)\n",
    "    x = vectorizer.fit_transform(urls)\n",
    "    \n",
    "    # Use stratified split to maintain class balance\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train the Logistic Regression model\n",
    "    lgr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    lgr.fit(x_train, y_train)\n",
    "    \n",
    "    # Calculate and print the score\n",
    "    score = lgr.score(x_test, y_test)\n",
    "    print(\"Score: {0:.2f} %\".format(100 * score))\n",
    "\n",
    "    # Save the vectorizer if needed\n",
    "    vectorizer_save = vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2996b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"C:\\Users\\Sai keerthan\\Downloads\\malware_detection_url\\Malware-Detection-using-Machine-learning-main\\Classifier\\pickel_model.pkl\"\n",
    "with open(file, 'wb') as f:\n",
    "    pickle.dump(lgr, f)\n",
    "f.close()\n",
    "\n",
    "file2 = r\"C:\\Users\\Sai keerthan\\Downloads\\malware_detection_url\\Malware-Detection-using-Machine-learning-main\\Classifier\\pickel_vector.pkl\"\n",
    "with open(file2,'wb') as f2:\n",
    "    pickle.dump(vectorizer_save, f2)\n",
    "f2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
